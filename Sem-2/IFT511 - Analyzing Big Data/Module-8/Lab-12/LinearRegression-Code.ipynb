{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0234d5ec-7703-444f-b818-21a2099c1f78",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-0.10992653682440251,-0.0,0.82061448078257,8.882098162004107,0.0,0.0,0.0,9.152366318770673]\nIntercept: 6.38886342311943\nnumIterations: 10\nobjectiveHistory: [0.5, 0.4512890716752172, 0.37167888074274397, 0.3640336879910583, 0.3593702890691326, 0.35814990628690374, 0.35621655813098696, 0.35385432692801094, 0.35095336145466255, 0.34816820384107144, 0.34766139530019635]\n+--------------------+\n|           residuals|\n+--------------------+\n|   6.204884555013344|\n| -0.9364542005967254|\n| -0.6347486102116999|\n|  0.8926597785593664|\n|-0.48228850573849336|\n| -0.2473514405238646|\n|   9.137839031509671|\n|   5.992350983065894|\n|-0.20296695703225254|\n|   8.208847332677813|\n|   4.353665478209582|\n|  1.2212477945765805|\n|  1.4711967585706134|\n| 0.33450145697385025|\n|  0.9582739246785774|\n|   2.041576643826449|\n| -1.0963563376943188|\n|  1.4739632894227022|\n|  -1.146822642963281|\n|   0.517734536995512|\n+--------------------+\nonly showing top 20 rows\n\nRMSE: 2.514112\nr2: 0.391812\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Load training data\n",
    "training = spark.read.format(\"libsvm\")\\\n",
    "    .load(\"/FileStore/tables/abalone.txt\")\n",
    "\n",
    "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: %s\" % str(lrModel.coefficients))\n",
    "print(\"Intercept: %s\" % str(lrModel.intercept))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "trainingSummary.residuals.show()\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "539385da-6354-4102-878c-a91fc125c38b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-0.1253922885274244,0.0,0.0,7.4212782992679,0.0,0.0,0.0,10.429449177242825]\nIntercept: 6.652621037793955\nnumIterations: 20\nobjectiveHistory: [0.5, 0.4512890716752172, 0.37167888074274397, 0.3640336879910583, 0.3593702890691326, 0.35814990628690374, 0.35621655813098696, 0.35385432692801094, 0.35095336145466255, 0.34816820384107144, 0.34766139530019635, 0.3474669492381573, 0.34738257665192573, 0.34736712080643645, 0.3472935854387981, 0.34725390189862326, 0.3472264344183484, 0.34722121809593004, 0.3472148486246941, 0.3472084744419787, 0.34720292160184996]\n+--------------------+\n|           residuals|\n+--------------------+\n|   6.203332435716595|\n| -0.9252052386076395|\n| -0.5938933583612673|\n|  0.9285468408523432|\n|-0.44376614090146926|\n|  -0.232999511911272|\n|   9.043253565880576|\n|   5.958846965769272|\n|-0.17574765092008526|\n|   8.147548057653005|\n|   4.369000250142394|\n|  1.2484549988862188|\n|  1.4893033366561657|\n|  0.3840411045322689|\n|  0.9265876115441802|\n|   2.004937269290364|\n|  -1.106639483032378|\n|  1.5002073162925367|\n|  -1.163875930932246|\n|  0.5312567654237537|\n+--------------------+\nonly showing top 20 rows\n\nRMSE: 2.505995\nr2: 0.395733\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Load training data\n",
    "training = spark.read.format(\"libsvm\")\\\n",
    "    .load(\"/FileStore/tables/abalone.txt\")\n",
    "\n",
    "lr = LinearRegression(maxIter=20, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: %s\" % str(lrModel.coefficients))\n",
    "print(\"Intercept: %s\" % str(lrModel.intercept))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "trainingSummary.residuals.show()\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f4e51a2-a3ae-4658-b79b-e162db51e153",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-0.13836570895300174,0.0,0.0,7.87091181601126,0.0,0.0,0.0,10.319673110914723]\nIntercept: 6.64147684085152\nnumIterations: 43\nobjectiveHistory: [0.5, 0.4512890716752172, 0.37167888074274397, 0.3640336879910583, 0.3593702890691326, 0.35814990628690374, 0.35621655813098696, 0.35385432692801094, 0.35095336145466255, 0.34816820384107144, 0.34766139530019635, 0.3474669492381573, 0.34738257665192573, 0.34736712080643645, 0.3472935854387981, 0.34725390189862326, 0.3472264344183484, 0.34722121809593004, 0.3472148486246941, 0.3472084744419787, 0.34720292160184996, 0.3471977002191727, 0.34719091922053447, 0.3471905408578589, 0.34719012166357993, 0.34719005927765095, 0.34719002020033934, 0.34719001756420165, 0.3471900145621974, 0.3471900136981129, 0.34719001331375815, 0.34719001312743086, 0.347190013046905, 0.34719001301726277, 0.34719001299844937, 0.34719001298680874, 0.3471900129819046, 0.3471900129795122, 0.3471900129778322, 0.3471900129768865, 0.34719001297406965, 0.3471900129734587, 0.34719001297310675, 0.347190012973018]\n+--------------------+\n|           residuals|\n+--------------------+\n|   6.201201278943204|\n| -0.9338703131035624|\n|  -0.594449871399128|\n|  0.9134755589082921|\n| -0.4236346803737252|\n|-0.21247710982335022|\n|   9.049125678050936|\n|   5.968275591215248|\n|-0.18972117220085494|\n|   8.152322409160083|\n|   4.366195569520816|\n|  1.2379326983667553|\n|  1.4735778818661647|\n| 0.37843937599533284|\n|  0.9390238699341342|\n|   1.996948785400484|\n| -1.0821696261086657|\n|   1.506605891034443|\n| -1.1647513882708918|\n|  0.5230352787451622|\n+--------------------+\nonly showing top 20 rows\n\nRMSE: 2.504686\nr2: 0.396364\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Load training data\n",
    "training = spark.read.format(\"libsvm\")\\\n",
    "    .load(\"/FileStore/tables/abalone.txt\")\n",
    "\n",
    "lr = LinearRegression(maxIter=50, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: %s\" % str(lrModel.coefficients))\n",
    "print(\"Intercept: %s\" % str(lrModel.intercept))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "trainingSummary.residuals.show()\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77e23c35-e371-4a09-88cf-8803a774b590",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 2.33245\nDecisionTreeRegressionModel: uid=DecisionTreeRegressor_ff0914474d97, depth=5, numNodes=63, numFeatures=8\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Load the data stored in LIBSVM format as a DataFrame.\n",
    "data = spark.read.format(\"libsvm\").load(\"/FileStore/tables/abalone.txt\")\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeRegressor(featuresCol=\"indexedFeatures\")\n",
    "\n",
    "# Chain indexer and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, dt])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "treeModel = model.stages[1]\n",
    "# summary only\n",
    "print(treeModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "267227b4-e6f9-4b9b-86ab-5ff45395e6b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Root Mean Squared Error (RMSE) over 10 runs = 2.34956\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Load the data stored in LIBSVM format as a DataFrame.\n",
    "data = spark.read.format(\"libsvm\").load(\"/FileStore/tables/abalone.txt\")\n",
    "\n",
    "# Initialize an empty list to store RMSE values\n",
    "rmseValues = []\n",
    "\n",
    "# Run the regression model learning 10 times\n",
    "for _ in range(10):\n",
    "    # Automatically identify categorical features, and index them.\n",
    "    # We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "    featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "\n",
    "    # Split the data into training and test sets (30% held out for testing)\n",
    "    (trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "    # Train a DecisionTree model.\n",
    "    dt = DecisionTreeRegressor(featuresCol=\"indexedFeatures\")\n",
    "\n",
    "    # Chain indexer and tree in a Pipeline\n",
    "    pipeline = Pipeline(stages=[featureIndexer, dt])\n",
    "\n",
    "    # Train model. This also runs the indexer.\n",
    "    model = pipeline.fit(trainingData)\n",
    "\n",
    "    # Make predictions.\n",
    "    predictions = model.transform(testData)\n",
    "\n",
    "    # Compute RMSE\n",
    "    evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    rmseValues.append(rmse)\n",
    "\n",
    "# Calculate the average RMSE\n",
    "averageRmse = sum(rmseValues) / len(rmseValues)\n",
    "print(\"Average Root Mean Squared Error (RMSE) over 10 runs = %g\" % averageRmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48f4a228-bf8e-400b-ba86-e74af234fffc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 2.63344\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Load data\n",
    "data = spark.read.format(\"libsvm\").load(\"/FileStore/tables/abalone.txt\")\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Define Linear Regression model\n",
    "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model on training data\n",
    "lrModel = lr.fit(trainingData)\n",
    "\n",
    "# Make predictions on test data\n",
    "predictions = lrModel.transform(testData)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "# Print the RMSE on test data\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e76e459f-5e5e-49de-81fc-2c3c9db8754d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Root Mean Squared Error (RMSE) over 10 runs = 2.51194\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Load data\n",
    "data = spark.read.format(\"libsvm\").load(\"/FileStore/tables/abalone.txt\")\n",
    "\n",
    "# Define Linear Regression model\n",
    "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Initialize a list to store RMSE values\n",
    "rmse_values_sum = 0\n",
    "\n",
    "# Run the regression model learning 10 times\n",
    "for i in range(10):\n",
    "    # Split the data into training and test sets (30% held out for testing)\n",
    "    (trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "    # Fit the model on training data\n",
    "    lrModel = lr.fit(trainingData)\n",
    "\n",
    "    # Make predictions on test data\n",
    "    predictions = lrModel.transform(testData)\n",
    "\n",
    "    # Select (prediction, true label) and compute test error\n",
    "    evaluator = RegressionEvaluator(\n",
    "        labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "    # Add the RMSE to the list\n",
    "    rmse_values_sum += rmse\n",
    "\n",
    "# Calculate the average RMSE\n",
    "averageRmse = rmse_values_sum / 10\n",
    "\n",
    "# Print the average RMSE\n",
    "print(\"Average Root Mean Squared Error (RMSE) over 10 runs = %g\" % averageRmse)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "LinearRegression-Code",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
